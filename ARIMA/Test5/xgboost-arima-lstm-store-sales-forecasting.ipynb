{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The notebook is structured as follows:\n",
    "\n",
    "**Import Libraries**: Đây là phần nhập các thư viện và module Python cần thiết để thao tác dữ liệu, trực quan hóa và học máy.\n",
    "\n",
    "**Load Datasets**: Năm bộ dữ liệu khác nhau liên quan đến doanh số cửa hàng, thông tin cửa hàng, giá dầu, sự kiện lễ hội và giao dịch được tải vào trong bước này.\n",
    "\n",
    "**Preprocessing**: Bước này chuyển đổi dữ liệu raw thành định dạng dễ tiêu thụ hơn cho các thuật toán học máy. Chúng ta thực hiện một loạt các công việc tiền xử lý như chuyển đổi cột ngày thành đối tượng datetime, mã hóa các đặc trưng hạng mục, kết hợp dữ liệu từ các bộ dữ liệu khác nhau, xử lý giá trị thiếu và tạo ra các đặc trưng mới.\n",
    "\n",
    "**Model Building & Evaluation**: Ở đây, chúng ta huấn luyện bốn mô hình khác nhau: XGBoost, ARIMA, LSTM và Prophet. Mỗi mô hình được chọn vì điểm mạnh cụ thể trong xử lý dữ liệu chuỗi thời gian. Sau khi huấn luyện, hiệu suất của mỗi mô hình được đánh giá bằng độ đo Mean Squared Error (MSE).\n",
    "\n",
    "**Submission**: Dựa trên hiệu suất của các mô hình, chúng ta chọn mô hình tốt nhất và sử dụng nó để dự đoán trên tập dữ liệu kiểm tra. Cuối cùng, chúng ta chuẩn bị một tệp CSV để gửi, bao gồm các dự đoán doanh số cho các ngày cụ thể."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-22T13:14:19.161161Z",
     "iopub.status.busy": "2023-05-22T13:14:19.160714Z",
     "iopub.status.idle": "2023-05-22T13:14:19.172442Z",
     "shell.execute_reply": "2023-05-22T13:14:19.171438Z",
     "shell.execute_reply.started": "2023-05-22T13:14:19.161118Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder, PolynomialFeatures\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:14:19.177964Z",
     "iopub.status.busy": "2023-05-22T13:14:19.176563Z",
     "iopub.status.idle": "2023-05-22T13:14:21.350863Z",
     "shell.execute_reply": "2023-05-22T13:14:21.349813Z",
     "shell.execute_reply.started": "2023-05-22T13:14:19.177915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv(\"E:/Jupyter NoteBook/XGBoost2/train.csv\")\n",
    "stores_df = pd.read_csv(\"E:/Jupyter NoteBook/XGBoost2/stores.csv\")\n",
    "oil_df = pd.read_csv(\"E:/Jupyter NoteBook/XGBoost2/oil.csv\")\n",
    "holidays_df = pd.read_csv(\"E:/Jupyter NoteBook/XGBoost2/holidays_events.csv\")\n",
    "transactions_df = pd.read_csv(\"E:/Jupyter NoteBook/XGBoost2/transactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:14:21.352987Z",
     "iopub.status.busy": "2023-05-22T13:14:21.352619Z",
     "iopub.status.idle": "2023-05-22T13:14:43.550092Z",
     "shell.execute_reply": "2023-05-22T13:14:43.54891Z",
     "shell.execute_reply.started": "2023-05-22T13:14:21.352947Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert date columns to datetime objects\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "oil_df['date'] = pd.to_datetime(oil_df['date'])\n",
    "holidays_df['date'] = pd.to_datetime(holidays_df['date'])\n",
    "transactions_df['date'] = pd.to_datetime(transactions_df['date'])\n",
    "\n",
    "# Merge train data with store, oil, and transactions data\n",
    "train_df = train_df.merge(stores_df, on='store_nbr', how='left')\n",
    "train_df = train_df.merge(oil_df, on='date', how='left')\n",
    "train_df = train_df.merge(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "# Encode categorical features\n",
    "le = LabelEncoder()\n",
    "train_df['city'] = le.fit_transform(train_df['city'])\n",
    "train_df['state'] = le.fit_transform(train_df['state'])\n",
    "train_df['type'] = le.fit_transform(train_df['type'])\n",
    "\n",
    "# Create holiday features\n",
    "holidays_df['holiday'] = 1\n",
    "train_df = train_df.merge(holidays_df[['date', 'holiday']], on='date', how='left')\n",
    "train_df['holiday'].fillna(0, inplace=True)\n",
    "\n",
    "# One-hot encode the 'family' column\n",
    "ohe_family = OneHotEncoder(sparse=False, dtype=int)\n",
    "family_encoded = ohe_family.fit_transform(train_df[['family']])\n",
    "family_columns = ohe_family.get_feature_names_out(['family'])\n",
    "family_df = pd.DataFrame(family_encoded, columns=family_columns, index=train_df.index)\n",
    "train_df = pd.concat([train_df, family_df], axis=1)\n",
    "\n",
    "# Drop the original 'family' column\n",
    "train_df.drop('family', axis=1, inplace=True)\n",
    "\n",
    "# Create lag features for oil prices\n",
    "lags = [1, 7, 14]\n",
    "for lag in lags:\n",
    "    train_df[f'oil_lag_{lag}'] = train_df['dcoilwtico'].shift(lag)\n",
    "\n",
    "# Create a feature for payday (every 15th and last day of the month)\n",
    "train_df['payday'] = ((train_df['date'].dt.day == 15) | (train_df['date'].dt.is_month_end)).astype(int)\n",
    "\n",
    "# Add date, month, year as features\n",
    "train_df['day'] = train_df['date'].dt.day\n",
    "train_df['month'] = train_df['date'].dt.month\n",
    "train_df['year'] = train_df['date'].dt.year\n",
    "\n",
    "# Add weekdays as one hot encoded features\n",
    "train_df['weekday'] = train_df['date'].dt.day_name()\n",
    "ohe = OneHotEncoder(sparse=False, dtype=int)\n",
    "weekday_encoded = ohe.fit_transform(train_df[['weekday']])\n",
    "weekday_columns = ohe.get_feature_names_out(['weekday'])\n",
    "weekday_df = pd.DataFrame(weekday_encoded, columns=weekday_columns, index=train_df.index)\n",
    "train_df = pd.concat([train_df, weekday_df], axis=1)\n",
    "\n",
    "# Drop the original 'family' column\n",
    "train_df.drop('weekday', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Drop rows with missing values\n",
    "train_df.dropna(inplace=True)\n",
    "\n",
    "# Add polynomial features\n",
    "poly_features = ['day', 'month', 'year', 'oil_lag_1', 'oil_lag_7', 'oil_lag_14']\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "train_df_poly = poly.fit_transform(train_df[poly_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:14:43.558081Z",
     "iopub.status.busy": "2023-05-22T13:14:43.556788Z",
     "iopub.status.idle": "2023-05-22T13:14:43.590183Z",
     "shell.execute_reply": "2023-05-22T13:14:43.588899Z",
     "shell.execute_reply.started": "2023-05-22T13:14:43.558032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>dcoilwtico</th>\n",
       "      <th>...</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>weekday_Friday</th>\n",
       "      <th>weekday_Monday</th>\n",
       "      <th>weekday_Saturday</th>\n",
       "      <th>weekday_Sunday</th>\n",
       "      <th>weekday_Thursday</th>\n",
       "      <th>weekday_Tuesday</th>\n",
       "      <th>weekday_Wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>1796</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>93.14</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>1797</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>93.14</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>1798</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>93.14</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>1799</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>93.14</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>1800</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>93.14</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       date  store_nbr  sales  onpromotion  city  state  type   \n",
       "1796  1796 2013-01-02          1    3.0            0    18     12     3  \\\n",
       "1797  1797 2013-01-02          1    0.0            0    18     12     3   \n",
       "1798  1798 2013-01-02          1    0.0            0    18     12     3   \n",
       "1799  1799 2013-01-02          1    0.0            0    18     12     3   \n",
       "1800  1800 2013-01-02          1    0.0            0    18     12     3   \n",
       "\n",
       "      cluster  dcoilwtico  ...  day  month  year  weekday_Friday   \n",
       "1796       13       93.14  ...    2      1  2013               0  \\\n",
       "1797       13       93.14  ...    2      1  2013               0   \n",
       "1798       13       93.14  ...    2      1  2013               0   \n",
       "1799       13       93.14  ...    2      1  2013               0   \n",
       "1800       13       93.14  ...    2      1  2013               0   \n",
       "\n",
       "      weekday_Monday  weekday_Saturday  weekday_Sunday  weekday_Thursday   \n",
       "1796               0                 0               0                 0  \\\n",
       "1797               0                 0               0                 0   \n",
       "1798               0                 0               0                 0   \n",
       "1799               0                 0               0                 0   \n",
       "1800               0                 0               0                 0   \n",
       "\n",
       "      weekday_Tuesday  weekday_Wednesday  \n",
       "1796                0                  1  \n",
       "1797                0                  1  \n",
       "1798                0                  1  \n",
       "1799                0                  1  \n",
       "1800                0                  1  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:14:43.596409Z",
     "iopub.status.busy": "2023-05-22T13:14:43.594356Z",
     "iopub.status.idle": "2023-05-22T13:14:46.449105Z",
     "shell.execute_reply": "2023-05-22T13:14:46.448016Z",
     "shell.execute_reply.started": "2023-05-22T13:14:43.596368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(train_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Define the features and target variable\n",
    "X_train = train_data.drop(['date', 'store_nbr', 'sales', 'dcoilwtico'], axis=1)\n",
    "y_train = train_data['sales']\n",
    "X_test = test_data.drop(['date', 'store_nbr', 'sales', 'dcoilwtico'], axis=1)\n",
    "y_test = test_data['sales']\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:14:46.451405Z",
     "iopub.status.busy": "2023-05-22T13:14:46.450724Z",
     "iopub.status.idle": "2023-05-22T13:14:56.35451Z",
     "shell.execute_reply": "2023-05-22T13:14:56.352672Z",
     "shell.execute_reply.started": "2023-05-22T13:14:46.451359Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost** & **ARIMA** Compared\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:14:56.356813Z",
     "iopub.status.busy": "2023-05-22T13:14:56.356397Z",
     "iopub.status.idle": "2023-05-22T13:15:45.144521Z",
     "shell.execute_reply": "2023-05-22T13:15:45.143447Z",
     "shell.execute_reply.started": "2023-05-22T13:14:56.356768Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Define the XGBoost model\n",
    "model_xg = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method='hist' \n",
    ")\n",
    "\n",
    "# Define the ARIMA model\n",
    "model_ar = ARIMA(y_train, order=(2, 0, 0))\n",
    "\n",
    "# Fit the models\n",
    "model_xg.fit(X_train, y_train)\n",
    "model_ar = model_ar.fit()  \n",
    "\n",
    "# Make predictions\n",
    "predictions_xg = model_xg.predict(X_test)\n",
    "predictions_ar = model_ar.predict(start=len(y_train), end=len(y_train) + len(y_test) - 1)  \n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse_xg = mean_squared_error(y_test, predictions_xg)\n",
    "mse_ar = mean_squared_error(y_test, predictions_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:15:45.146451Z",
     "iopub.status.busy": "2023-05-22T13:15:45.14606Z",
     "iopub.status.idle": "2023-05-22T13:15:45.156823Z",
     "shell.execute_reply": "2023-05-22T13:15:45.155717Z",
     "shell.execute_reply.started": "2023-05-22T13:15:45.146408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102475.99439151309\n",
      "1011636.7909510806\n"
     ]
    }
   ],
   "source": [
    "print(mse_xg)\n",
    "print(mse_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Results\n",
    "\n",
    "LSTM is a type of Recurrent Neural Network (RNN) that is very effective in predicting time series data because it can remember patterns over the time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:15:45.162041Z",
     "iopub.status.busy": "2023-05-22T13:15:45.161208Z",
     "iopub.status.idle": "2023-05-22T13:15:45.170464Z",
     "shell.execute_reply": "2023-05-22T13:15:45.169311Z",
     "shell.execute_reply.started": "2023-05-22T13:15:45.162Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# Reshape input to be 3D [samples, timesteps, features]\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:15:45.172682Z",
     "iopub.status.busy": "2023-05-22T13:15:45.172062Z",
     "iopub.status.idle": "2023-05-22T13:15:45.184406Z",
     "shell.execute_reply": "2023-05-22T13:15:45.183327Z",
     "shell.execute_reply.started": "2023-05-22T13:15:45.172627Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:15:45.186493Z",
     "iopub.status.busy": "2023-05-22T13:15:45.185813Z",
     "iopub.status.idle": "2023-05-22T13:24:10.321563Z",
     "shell.execute_reply": "2023-05-22T13:24:10.320416Z",
     "shell.execute_reply.started": "2023-05-22T13:15:45.186454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "21443/21443 - 115s - loss: 275.7538 - val_loss: 236.5224 - 115s/epoch - 5ms/step\n",
      "Epoch 2/5\n",
      "21443/21443 - 109s - loss: 216.1333 - val_loss: 200.0111 - 109s/epoch - 5ms/step\n",
      "Epoch 3/5\n",
      "21443/21443 - 105s - loss: 188.1707 - val_loss: 178.2664 - 105s/epoch - 5ms/step\n",
      "Epoch 4/5\n",
      "21443/21443 - 110s - loss: 171.1525 - val_loss: 164.9102 - 110s/epoch - 5ms/step\n",
      "Epoch 5/5\n",
      "21443/21443 - 114s - loss: 160.1946 - val_loss: 155.6198 - 114s/epoch - 5ms/step\n",
      "12062/12062 [==============================] - 37s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(50, activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0, unroll=False, use_bias=True, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "history = model_lstm.fit(X_train_lstm, y_train, epochs=5, batch_size=72,\n",
    "                         validation_data=(X_test_lstm, y_test), verbose=2, shuffle=False)\n",
    "\n",
    "# make a prediction\n",
    "predictions_lstm = model_lstm.predict(X_test_lstm)\n",
    "\n",
    "# flatten predictions\n",
    "predictions_lstm = predictions_lstm.flatten()\n",
    "\n",
    "# calculate the mean squared error\n",
    "mse_lstm = mean_squared_error(y_test, predictions_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:24:10.323633Z",
     "iopub.status.busy": "2023-05-22T13:24:10.323218Z",
     "iopub.status.idle": "2023-05-22T13:24:10.329203Z",
     "shell.execute_reply": "2023-05-22T13:24:10.32808Z",
     "shell.execute_reply.started": "2023-05-22T13:24:10.323589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460182.6288739181\n"
     ]
    }
   ],
   "source": [
    "print(mse_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet results\n",
    "\n",
    "Prophet is an open-source library developed by Facebook that is specifically designed for forecasting time series data. It's robust to missing data, shifts in the trend, and large outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:30:50.412941Z",
     "iopub.status.busy": "2023-05-22T13:30:50.411938Z",
     "iopub.status.idle": "2023-05-22T13:30:50.434578Z",
     "shell.execute_reply": "2023-05-22T13:30:50.433518Z",
     "shell.execute_reply.started": "2023-05-22T13:30:50.412881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:52:36.260819Z",
     "iopub.status.busy": "2023-05-22T13:52:36.259877Z",
     "iopub.status.idle": "2023-05-22T13:52:38.447951Z",
     "shell.execute_reply": "2023-05-22T13:52:38.44685Z",
     "shell.execute_reply.started": "2023-05-22T13:52:36.260777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose the model with the lowest mean squared error\n",
    "if min(mse_xg, mse_ar, mse_lstm) == mse_xg:\n",
    "    model = model_xg\n",
    "elif min(mse_xg, mse_ar, mse_lstm) == mse_ar:\n",
    "    model = model_ar\n",
    "else:\n",
    "    model = model_lstm\n",
    "\n",
    "# Make predictions using the chosen model\n",
    "if model == model_lstm:\n",
    "    X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    predictions = model.predict(X_test_lstm)\n",
    "else:\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "predictions = predictions.reshape(-1, 1)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission = pd.DataFrame({\"sales\": predictions.flatten()})\n",
    "# Assume that 'id' values are a range from 1 to the length of predictions\n",
    "submission = pd.DataFrame({\"id\": range(1, len(predictions) + 1), \"sales\": predictions.flatten()})\n",
    "\n",
    "# Save the submission DataFrame as a CSV file\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:52:40.222252Z",
     "iopub.status.busy": "2023-05-22T13:52:40.221745Z",
     "iopub.status.idle": "2023-05-22T13:52:40.233987Z",
     "shell.execute_reply": "2023-05-22T13:52:40.231931Z",
     "shell.execute_reply.started": "2023-05-22T13:52:40.222204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2397228. ],\n",
       "       [13189131. ],\n",
       "       [-3814958.2],\n",
       "       ...,\n",
       "       [ 8697059. ],\n",
       "       [19096656. ],\n",
       "       [ -343371.1]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-22T13:52:49.981002Z",
     "iopub.status.busy": "2023-05-22T13:52:49.980448Z",
     "iopub.status.idle": "2023-05-22T13:52:50.001571Z",
     "shell.execute_reply": "2023-05-22T13:52:50.000483Z",
     "shell.execute_reply.started": "2023-05-22T13:52:49.980967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.397228e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.318913e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-3.814958e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.417156e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.509830e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385949</th>\n",
       "      <td>385950</td>\n",
       "      <td>5.982828e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385950</th>\n",
       "      <td>385951</td>\n",
       "      <td>9.789154e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385951</th>\n",
       "      <td>385952</td>\n",
       "      <td>-2.461750e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385952</th>\n",
       "      <td>385953</td>\n",
       "      <td>8.590516e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385953</th>\n",
       "      <td>385954</td>\n",
       "      <td>1.827505e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>385954 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id         sales\n",
       "0            1  2.397228e+06\n",
       "1            2  1.318913e+07\n",
       "2            3 -3.814958e+06\n",
       "3            4  3.417156e+06\n",
       "4            5  3.509830e+07\n",
       "...        ...           ...\n",
       "385949  385950  5.982828e+05\n",
       "385950  385951  9.789154e+07\n",
       "385951  385952 -2.461750e+06\n",
       "385952  385953  8.590516e+07\n",
       "385953  385954  1.827505e+07\n",
       "\n",
       "[385954 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223.865px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
